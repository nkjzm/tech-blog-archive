---
title: 【wget】Webサイトを丸ごとローカル環境にダウンロードする備忘録
tags:
  - Linux
private: false
updated_at: '2019-10-26T10:04:00+09:00'
id: aa60bfa58a854e59e579
organization_url_name: null
slide: false
ignorePublish: false
---
# はじめに

管理者画面にアクセスできなくなったワードプレスのサイトがあり、保存しておきたい場面がありました。その方法とハマりどころをまとめます。

# 環境

- Mac OSX 10.15（19A602）

# wgetの準備

Webサーバーからコンテンツをダウンロードするためのコマンドです。

```.bash
$ brew install wget
```

# Webサイトの一括ダウンロード

```.bash
$ wget -rkp -l 3 http://example.com/
```

- `-r`, `--recursive`: 再帰ダウンロードを行う
- `-k`, `--convert-links`: HTML や CSS 中のリンクをローカルを指すように変更する
    - 変更されるのは全てのダウンロードが終了後に行われるので、**ダウンロード中に確認しても適用されていない点に注意**
- `-p`, `--page-requisites`: HTML を表示するのに必要な全ての画像等も取得する
- `-l`, `--level=NUMBER`: 再帰時の階層の最大の深さを NUMBER に設定する (0 で無制限)

## 課題

再起的に保存する`.css`や`.js`にURLパラメータが付いていることがあり、URLパラメータ付きの名前として保存されてしまっていた。保存時に`?`や`&`はエスケープされてしまうためローカルHTML上からアクセスできない問題が発生した。

参照が外れている`.css`, `.js`ファイルをリネーム(`?`以下を削除)することで解決した。
(数が多くなかったので手動で対応したが、多い場合は正規表現などで置換できそう。)

## 試したが使わなかったコマンド

- `-H`, `--span-hosts`: 再帰中に別のホストもダウンロード対象にする
    - あまり外部ドメインに依存していなかった
- `--content-disposition`: Content-Dispositionで指定されたファイル名で保存する

## 最後に

`-k`の適用タイミングで結構ハマりました。
